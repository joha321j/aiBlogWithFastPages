{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Week 8\n",
    "> Answers to the exercises for week 8\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does pixel similarity approach to classifying digits works?\n",
    "Opstil matrix, der repræsenterer billedet, hvert koordinat repræsenterer et pixel.\n",
    "Man kan nu sammenligne denne matrice med en idéel udgave af det, som man skal klassificere mod, ved at se på, hvor stor forskel der er på den idéelle matrix og billedmatrixen. Forskellen udregnes typisk med root mean square (L2 norm) eller mean of absolute differences(L1 norm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 14, 18]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberList = [1, 2, 3, 4, 7, 8, 9]\n",
    "evenNumbers = [number * 2 for number in numberList if number % 2 != 0]\n",
    "evenNumbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a rank-3 tensor?\n",
    "En tensors rank beskriver hvor mange unikke datatyper den tensor indeholder. Dette har ikke noget med størrelsen eller mængden af data at gøre, men hvad dataen beskriver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are RMSE and L1 norm?\n",
    "Root mean square error - tag kvadratet af forskellen inden man tager gennemsnittet og tag derefter kvadratroden af gennemsnittet\n",
    "mean of absolute differences - tag den absolute værdi af forskellene inden man tager gennensmittet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 12],\n",
       "        [16, 18]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "\n",
    "data = [[1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]]\n",
    "dataTensor = t.tensor(data)\n",
    "\n",
    "doubledTensor = dataTensor * 2\n",
    "doubledTensor\n",
    "doubledTensor[1:, 1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is broadcasting?\n",
    "Automatically expand the tensor with the smaller rank to have the same size as\n",
    "the one with the larger rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are metrics generally calculated using the training set, or the validation set? Why?\n",
    "Metrics udregnes på valideringssættet, da de fortæller os, hvor gode vores model er."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is SGD?\n",
    "Stochastic Gradient Descent\n",
    "En iterativ måde at optimere vores model via gradienter.\n",
    "Learning raten angiver hvor store skridt man ta'r mellem hver iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the seven steps in SGD for machine learning? \n",
    "1. Initialize the weights.\n",
    "2. For each image, use these weights to predict whether it appears to be a 3 or a 7.\n",
    "3. Based on these predictions, calculate how good the model is (its loss).\n",
    "4. Calculate the gradient, which measures for each weight how changing that weight would change the loss.\n",
    "5. Step (that is, change) all the weights based on that calculation.\n",
    "6. Go back to step 2 and repeat the process.\n",
    "7. Iterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we initialize the weights in a model? \n",
    "Randomly, fordi vi allerede ved, at vi har en måde at forbedre dem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " What is \"loss\"?\n",
    "    This is what Samuel referred to when he spoke of testing the effectiveness of any\n",
    "    current weight assignment in terms of actual performance. We need a function that\n",
    "    will return a number that is small if the performance of the model is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why can't we always use a high learning rate?\n",
    "Vi risikerere at oversteppe de idelle vægte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a \"gradient\"?\n",
    "Differentialet af metoden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why can't we use accuracy as a loss function?\n",
    "Små ændringer i vægtene vil ofte ikke have en indflydelse på vores accuracy, hvilket betyder at hvis accuracy bli'r brugt vil det være meget små gradientværdier og derfor vil modellen ikke lære noget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between a loss function and a metric?\n",
    "A loss function is used to train your model. A metric is used to evaluate your model.\n",
    "A loss function is used during the learning process. A metric is used after the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the function to calculate new weights using a learning rate? \n",
    "backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the backward method do?\n",
    "The “backward” here refers to backpropagation, which is the name given to the process\n",
    "of calculating the derivative of each layer. We’ll see how this is done exactly in\n",
    "Chapter 17, when we calculate the gradients of a deep neural net from scratch. This is\n",
    "called the backward pass of the network, as opposed to the forward pass, which is\n",
    "where the activations are calculated. Life would probably be easier if backward was\n",
    "just called calculate_grad, but deep learning folks really do like to add jargon everywhere\n",
    "they can!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
